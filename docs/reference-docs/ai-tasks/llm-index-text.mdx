---
sidebar_position: 8
slug: "/reference-docs/ai-tasks/llm-index-text"
description: "The LLM Index Text task indexes a provided text into a vector database."
---


import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# LLM Index Text

The LLM Index Text task is designed to index the provided text into a vector space for efficient search, retrieval, and processing at a later stage.

It takes text input, processes it using a specified language model to generate embeddings, and stores these embeddings in a chosen vector database. 

:::info Prerequisites

- [Integrate the required AI model](/category/integrations/ai-llm) with Orkes Conductor.
- [Integrate the required vector database](/category/integrations/vector-databases) with Orkes Conductor.
:::


## Task parameters

Configure these parameters for the LLM Index Text task.

| Parameter | Description | Required/ Optional | 
| --------- | ----------- | ----------------- |
| inputParameters.**vectorDB** | The vector database to store the data.<br/><br/>**Note**: If you haven’t configured the vector database on your Orkes Conductor cluster, navigate to the **Integrations** tab and [configure your required provider](/category/integrations/vector-databases). | Required. |
| inputParameters.**namespace** | Namespaces are separate isolated environments within the database to manage and organize vector data effectively. Enter the namespace the task will utilize. <br/><br/>The usage and terminology of the namespace field vary depending on the integration:<ul><li>For Pinecone, the namespace field is applicable.</li><li>For Weaviate, the namespace field is not applicable.</li><li>For MongoDB, the namespace field is referred to as “Collection” in MongoDB.</li><li>For Postgres, the namespace field is referred to as “Table” in Postgres.</li></ul> | Required. |
| inputParameters.**index** | The index in your vector database where the text or data will be stored.<br/><br/>The terminology of the index field varies depending on the integration:<ul><li>For Weaviate, the index field indicates the collection name.</li><li>For other integrations, it denotes the index name.</li></ul> | Required. | 
| inputParameters.**embeddingModelProvider** | The LLM provider for generating the embeddings.<br/><br/>**Note**: If you haven’t configured your AI/LLM provider on your Orkes Conductor cluster, navigate to the **Integrations** tab and [configure your required provider](/category/integrations/ai-llm). | Required. |
| inputParameters.**embeddingModel** | The embedding model provided by the selected LLM provider to generate the embeddings. | Required. |
| inputParameters.**dimensions** | The size of the vector, which is the number of elements in the vector. | Optional. | 
| inputParameters.**text** | The text to be indexed. | Required. |
| inputParameters.**docId** | A unique ID to identify the document where the indexed text will be stored. | Required. |

The following are generic configuration parameters that can be applied to the task and are not specific to the LLM Index Text task.

<details>
<summary>Caching parameters</summary>

You can cache the task outputs using the following parameters. Refer to [Caching Task Outputs](/faqs/task-cache-output) for a full guide.

| Parameter | Description | Required/ Optional | 
| --------- | ----------- | ----------------- | 
| cacheConfig.**ttlInSecond** | The time to live in seconds, which is the duration for the output to be cached. | Required if using *cacheConfig*. |
| cacheConfig.**key** | The cache key is a unique identifier for the cached output and must be constructed exclusively from the task’s input parameters.<br/>It can be a string concatenation that contains the task’s input keys, such as `${uri}-${method}` or `re_${uri}_${method}`. | Required if using *cacheConfig*. |

</details>

<details>
<summary>Other generic parameters</summary>

Here are other parameters for configuring the task behavior.

| Parameter | Description | Required/ Optional | 
| --------- | ----------- | ----------------- | 
| optional | Whether the task is optional. <br/><br/>If set to`true`, any task failure is ignored, and the workflow continues with the task status updated to `COMPLETED_WITH_ERRORS`. However, the task must reach a terminal state. If the task remains incomplete, the workflow waits until it reaches a terminal state before proceeding. | Optional. | 

</details>

## Task configuration

This is the task configuration for an LLM Index Text task.

```json
{
     "name": "llm_index_text",
     "taskReferenceName": "llm_index_text_ref",
     "inputParameters": {
       "vectorDB": "Pinecone",
       "index": "doc",
       "namespace": "docs",
       "embeddingModelProvider": "openAI",
       "embeddingModel": "text-embedding-3-large",
       "dimensions": 3024,
       "text": "${workflow.input.text}",
       "docId": "doc123"
     },
     "type": "LLM_INDEX_TEXT"
}
```

## Task output

There is no output. The LLM Index Text task will store the indexed data in the specified vector database. 

## Adding an LLM Index Document task in UI

**To add an LLM Index Document task:**

1. In your workflow, select the (+) icon and add an **LLM Index Document** task.
2. In **Vector Database Configuration**, select the **Vector database**, **Index**, and **Namespace** to store the indexed text.
3. In **Embedding Model**, select the **Embedding model provider**, **Embedding model**, and **Dimensions** to generate the embeddings.
4. Enter the **Text** to be indexed.
5. Enter an arbitrary **Doc ID** to store the indexed text.

<center><p><img src="/content/img/llm-index-text-ui-method.png" alt="LLM Index Text Task - UI" width="80%" height="auto"/></p></center>

## Examples

Here are some examples for using the LLM Index Text task.

<details>
<summary>Using an LLM Index Text task in a workflow</summary>

See an example of [building a text indexing and search workflow](http://orkes.io/content/tutorials/text-indexing-search-workflow).

</details>