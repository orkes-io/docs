---
sidebar_position: 10
slug: "/reference-docs/ai-tasks/llm-chat-complete"
description: "The LLM Chat Complete task is used instruct the LLM to behave in a certain manner."
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# LLM Chat Complete

The LLM Chat Complete task is used to complete a chat query based on additional instructions. It can be used to govern the model's behavior to minimize deviation from the intended objective.

The LLM Chat Complete task processes a chat query by taking the user's input and generating a response based on the supplied instructions and parameters. This helps the model to stay focused on the objective and provides control over the model's output behavior.

:::info Prerequisites

- [Integrate the required AI model](/category/integrations/ai-llm) with Orkes Conductor.
- [Create the required AI prompt](/content/developer-guides/creating-and-managing-gen-ai-prompt-templates) for the task.
:::

## Task parameters 

Configure these parameters for the LLM Chat Complete task.

| Parameter | Description | Required/ Optional | 
| --------- | ----------- | ----------------- |
| inputParameters. **llmProvider** | The integration name of the LLM provider integrated with your Conductor cluster.<br/><br/>**Note:** If you haven’t configured your AI/LLM provider on your Orkes Conductor cluster, go to the **Integrations** tab and [configure your required provider](/category/integrations/ai-llm). | Required. |
| inputParameters. **model** | The available language models within the selected LLM provider. <br/><br/>For example, If your LLM provider is Azure Open AI and you’ve configured text-davinci-003 as the language model, you can select it here. | Required. |
| inputParameters. **instructions** | Enter the ground rules or instructions for the chat so the model responds to only specific queries and will not deviate from the objective. Under this section, you can also save the instructions as an AI prompt in Orkes Conductor and add it here. <br/><br/>**Note**: If you haven’t created an AI prompt for your language model, refer to the documentation on [creating AI Prompts in Orkes Conductor](/developer-guides/creating-and-managing-gen-ai-prompt-templates). | Required. |
| inputParameters. **promptVariables** | For prompts that involve variables, provide the input to these variables within this field. It can be string, number, boolean, null, object/array. | Optional. |
| inputParameters. **messages** | The appropriate role and messages to complete the chat query. Supported values:<ul><li>role</li><li>message</li></ul> | Optional. |
| inputParameters. **messages.role**| The required role for the chat completion. Available options include _user_, _assistant_, _system_, or _human_.<ul><li>The roles “user” and “human” represent the user asking questions or initiating the conversation.</li><li>The roles “assistant” and “system” refer to the model responding to the user queries.</li></ul> | Optional. |
| inputParameters. **messages.message** | The corresponding input message to be provided. It can also be [passed as a dynamic input](/developer-guides/passing-inputs-to-task-in-conductor). | Optional. |
| inputParameters. **temperature** | A parameter to control the randomness of the model’s output. Higher temperatures, such as 1.0, make the output more random and creative. A lower value makes the output more deterministic and focused.<br/><br/>**Tip:** If you're using a text blurb as input and want to categorize it based on its content type, opt for a lower temperature setting. Conversely, if you're providing text inputs and intend to generate content like emails or blogs, it's advisable to use a higher temperature setting. | Optional. |
| inputParameters. **stopWords** | List of words to be omitted during text generation. Supports string and object/array.<br/><br/>In LLM, stop words may be filtered out or given less importance during the text generation process to ensure that the generated text is coherent and contextually relevant. | Optional. |
| inputParameters. **topP** | Another parameter to control the randomness of the model’s output. This parameter defines a probability threshold and then chooses tokens whose cumulative probability exceeds this threshold.<br/><br/>**Example**: Imagine you want to complete the sentence: “She walked into the room and saw a __.” The top few words the LLM model would consider based on the highest probabilities would be:<ul><li>Cat - 35%</li><li>Dog - 25%</li><li>Book - 15%</li><li>Chair - 10%</li></ul>If you set the top-p parameter to 0.70, the LLM model will consider tokens until their cumulative probability reaches or exceeds 70%. Here's how it works:<ol><li>Add "Cat" (35%) to the cumulative probability.</li><li>Add "Dog" (25%) to the cumulative probability, totaling 60%.</li><li>Add "Book" (15%) to the cumulative probability, now at 75%.</li></ol>At this point, the cumulative probability is 75%, exceeding the set top-p value of 70%. Therefore, the LLM will randomly select one of the tokens from the list of "Cat," "Dog," and "Book" to complete the sentence because these tokens collectively account for approximately 75% of the likelihood. | Optional. |
| inputParameters. **maxTokens** | The maximum number of tokens to be generated by the LLM and returned as part of the result. A token is approximately four characters. | Optional. |
| inputParameters. **jsonOutput** | Determines whether the LLM’s response is to be parsed as JSON. When set to ‘true’, the model’s response will be processed as structured JSON data. | Optional. | 

The following are generic configuration parameters that can be applied to the task and are not specific to the LLM Chat Complete task.

<details>
<summary>Caching parameters</summary>

You can cache the task outputs using the following parameters. Refer to [Caching Task Outputs](/faqs/task-cache-output) for a full guide.

| Parameter | Description | Required/ Optional | 
| --------- | ----------- | ----------------- | 
| cacheConfig.**ttlInSecond** | The time to live in seconds, which is the duration for the output to be cached. | Required if using *cacheConfig*. |
| cacheConfig.**key** | The cache key is a unique identifier for the cached output and must be constructed exclusively from the task’s input parameters.<br/>It can be a string concatenation that contains the task’s input keys, such as `${uri}-${method}` or `re_${uri}_${method}`. | Required if using *cacheConfig*. |

</details>

<details>
<summary>Schema parameters</summary>

You can enforce input/output validation for the task using the following parameters. Refer to [Schema Validation](/developer-guides/schema-validation) for a full guide.

| Parameter | Description | Required/ Optional | 
| --------- | ----------- | ----------------- | 
| taskDefinition.**enforceSchema** | Whether to enforce schema validation for task inputs/outputs. Set to *true* to enable validation. | Optional. | 
| taskDefinition.**inputSchema** | The name and type of the input schema to be associated with the task. | Required if *enforceSchema* is set to true. | 
| taskDefinition.**outputSchema** | The name and type of the output schema to be associated with the task. | Required if *enforceSchema* is set to true.

</details>

<details>
<summary>Other generic parameters</summary>

Here are other parameters for configuring the task behavior.

| Parameter | Description | Required/ Optional | 
| --------- | ----------- | ----------------- | 
| optional | Whether the task is optional. The default is false. <br/> <br/> If set to true, the workflow continues to the next task even if this task is in progress or fails.| Optional. | 

</details>

## Task configuration

This is the task configuration for an LLM Chat Complete task.

```json
{
  "name": "llm_chat_complete",
  "taskReferenceName": "llm_chat_complete_ref",
  "inputParameters": {
    "llmProvider": "openai",
    "model": "gpt-4",
    "instructions": "your-prompt-template", // Can be harcoded instructions or select the prompt here
    "messages": [
      {
        "role": "user",
        "message": "${workflow.input.text}"
      }
    ],
    "temperature": 0.1,
    "topP": 0.2,
    "maxTokens": 4,
    "stopWords": "spam",
    "promptVariables": {
      "text": "${workflow.input.text}",
      "language": "${workflow.input.language}"
    },
    "jsonOutput": true
  },
  "type": "LLM_CHAT_COMPLETE"
}
```

## Task output

The LLM Chat Complete task will return the following parameters.

| Parameter | Description | 
| --------- | ----------- | 
| result | The completed chat generated by the LLM. | 

## Adding an LLM Text Complete task in UI

**To add an LLM Text Complete task:**

1. In your workflow, select the (**+**) icon and add an **LLM Text Complete** task.
2. Choose the **LLM provider** and **Model**.
3. In the **Instructions** field, set the ground rules or instructions to ensure the model responds only to specific queries. You can save these instructions as an AI prompt and add them here.
4. (Optional) Click **+Add variable** to provide the variable path if your prompt template includes variables.
5. (Optional) Click **+Add message** and choose the appropriate role and messages to complete the chat query.
6. (Optional) Set the parameters **Temperature**, **Stop words**, **TopP**, and **Token Limit**.
7. (Optional) Enable **JSON output** to format the LLM’s response as a structured JSON.

<center><p><img src="/content/img/llm-chat-complete-ui-method.png" alt="LLM Chat Complete Task - UI" width="80%" height="auto"/></p></center>